{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild Gutenberg Poetry Corpus with Proper Lineation\n",
    "\n",
    "**Purpose:** Download and parse 1,191 Project Gutenberg poetry texts with proper line breaks, stanza markers, and structure.\n",
    "\n",
    "**Output:** Clean JSONL corpus saved to Google Drive\n",
    "\n",
    "**Time:** ~1-2 hours for 1,191 texts\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Upload `gutenberg_ids.txt` to Colab Files\n",
    "2. Run all cells in order\n",
    "3. Output will save to Google Drive: `/MyDrive/gutenberg_poetry_corpus_clean.jsonl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"✓ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Gutenberg ID List\n",
    "\n",
    "Upload `gutenberg_ids.txt` using the Files panel on the left, or run this cell to upload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Upload gutenberg_ids.txt:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify\n",
    "with open('gutenberg_ids.txt', 'r') as f:\n",
    "    ids = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"\\n✓ Loaded {len(ids)} Gutenberg IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests tqdm\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Corpus Rebuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class GutenbergCorpusRebuilder:\n",
    "    \"\"\"Rebuild corpus from original Gutenberg sources.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.failed_downloads = []\n",
    "\n",
    "    def download_text(self, gutenberg_id, max_retries=3):\n",
    "        \"\"\"Download text from Project Gutenberg with retries.\"\"\"\n",
    "\n",
    "        url_patterns = [\n",
    "            f\"http://www.gutenberg.org/cache/epub/{gutenberg_id}/pg{gutenberg_id}.txt\",\n",
    "            f\"https://www.gutenberg.org/files/{gutenberg_id}/{gutenberg_id}-0.txt\",\n",
    "            f\"https://www.gutenberg.org/files/{gutenberg_id}/{gutenberg_id}.txt\",\n",
    "        ]\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            for url in url_patterns:\n",
    "                try:\n",
    "                    response = requests.get(url, timeout=30)\n",
    "                    if response.status_code == 200:\n",
    "                        if not response.text.startswith('<!DOCTYPE'):\n",
    "                            return response.text\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "\n",
    "        logger.warning(f\"Failed to download ID {gutenberg_id}\")\n",
    "        self.failed_downloads.append(gutenberg_id)\n",
    "        return None\n",
    "\n",
    "    def clean_gutenberg_text(self, text):\n",
    "        \"\"\"Remove Gutenberg headers and footers.\"\"\"\n",
    "\n",
    "        start_markers = [\n",
    "            \"*** START OF THIS PROJECT GUTENBERG\",\n",
    "            \"*** START OF THE PROJECT GUTENBERG\",\n",
    "            \"*END*THE SMALL PRINT\",\n",
    "        ]\n",
    "\n",
    "        for marker in start_markers:\n",
    "            if marker in text:\n",
    "                text = text.split(marker, 1)[1]\n",
    "                break\n",
    "\n",
    "        end_markers = [\n",
    "            \"*** END OF THIS PROJECT GUTENBERG\",\n",
    "            \"*** END OF THE PROJECT GUTENBERG\",\n",
    "            \"End of the Project Gutenberg\",\n",
    "            \"End of Project Gutenberg\",\n",
    "        ]\n",
    "\n",
    "        for marker in end_markers:\n",
    "            if marker in text:\n",
    "                text = text.split(marker, 1)[0]\n",
    "                break\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def parse_poetry_lines(self, text, gutenberg_id):\n",
    "        \"\"\"\n",
    "        Parse text into poetry lines with metadata.\n",
    "        \n",
    "        Preserves:\n",
    "        - Line breaks (exact lineation)\n",
    "        - Blank lines (stanza breaks)\n",
    "        - Line numbers from source\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        text = self.clean_gutenberg_text(text)\n",
    "        raw_lines = text.split('\\n')\n",
    "\n",
    "        for line_num, line in enumerate(raw_lines, 1):\n",
    "            lines.append({\n",
    "                'line': line.strip(),\n",
    "                'gutenberg_id': gutenberg_id,\n",
    "                'line_num': line_num,\n",
    "                'is_blank': not line.strip()\n",
    "            })\n",
    "\n",
    "        return lines\n",
    "\n",
    "    def process_all(self, id_file, output_jsonl):\n",
    "        \"\"\"Process all Gutenberg IDs and create clean corpus.\"\"\"\n",
    "\n",
    "        with open(id_file, 'r') as f:\n",
    "            gutenberg_ids = [int(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "        logger.info(f\"Processing {len(gutenberg_ids)} Gutenberg texts\")\n",
    "\n",
    "        processed = 0\n",
    "        total_lines = 0\n",
    "\n",
    "        with open(output_jsonl, 'w') as outfile:\n",
    "            for gid in tqdm(gutenberg_ids, desc=\"Processing texts\"):\n",
    "                text = self.download_text(gid)\n",
    "\n",
    "                if text is None:\n",
    "                    continue\n",
    "\n",
    "                lines = self.parse_poetry_lines(text, gid)\n",
    "\n",
    "                for line_data in lines:\n",
    "                    outfile.write(json.dumps(line_data) + '\\n')\n",
    "                    total_lines += 1\n",
    "\n",
    "                processed += 1\n",
    "\n",
    "                if processed % 50 == 0:\n",
    "                    logger.info(f\"Progress: {processed}/{len(gutenberg_ids)} texts, {total_lines:,} lines\")\n",
    "\n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"Processing complete!\")\n",
    "        logger.info(f\"Texts processed: {processed}/{len(gutenberg_ids)}\")\n",
    "        logger.info(f\"Total lines: {total_lines:,}\")\n",
    "        logger.info(f\"Failed downloads: {len(self.failed_downloads)}\")\n",
    "\n",
    "        if self.failed_downloads:\n",
    "            failed_file = str(Path(output_jsonl).parent / \"failed_downloads.txt\")\n",
    "            with open(failed_file, 'w') as f:\n",
    "                for fid in self.failed_downloads:\n",
    "                    f.write(f\"{fid}\\n\")\n",
    "            logger.info(f\"Failed IDs saved to: {failed_file}\")\n",
    "\n",
    "        logger.info(f\"Clean corpus saved to: {output_jsonl}\")\n",
    "        \n",
    "        return processed, total_lines\n",
    "\n",
    "\n",
    "print(\"✓ Rebuilder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Corpus Rebuild\n",
    "\n",
    "**This will take 1-2 hours.** You can close the browser tab and it will keep running.\n",
    "\n",
    "Check back later to see progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize rebuilder\n",
    "rebuilder = GutenbergCorpusRebuilder()\n",
    "\n",
    "# Paths\n",
    "id_file = 'gutenberg_ids.txt'\n",
    "output_file = '/content/drive/MyDrive/AI and Poetry/Historical Embeddings/gutenberg_poetry_corpus_clean.jsonl'\n",
    "\n",
    "# Run rebuild\n",
    "print(\"Starting corpus rebuild...\")\n",
    "print(\"This will take 1-2 hours for 1,191 texts.\")\n",
    "print(\"You can close the browser - it will keep running.\\n\")\n",
    "\n",
    "processed, total_lines = rebuilder.process_all(id_file, output_file)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REBUILD COMPLETE!\")\n",
    "print(f\"Processed: {processed} texts\")\n",
    "print(f\"Total lines: {total_lines:,}\")\n",
    "print(f\"Output: {output_file}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Output\n",
    "\n",
    "Check first few lines and compare with original corrupted corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_file = '/content/drive/MyDrive/AI and Poetry/Historical Embeddings/gutenberg_poetry_corpus_clean.jsonl'\n",
    "\n",
    "print(\"First 10 lines of rebuilt corpus:\\n\")\n",
    "with open(output_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 10:\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        blank = \"[BLANK LINE]\" if data['is_blank'] else \"\"\n",
    "        print(f\"{i+1}. ID {data['gutenberg_id']} Line {data['line_num']}: {data['line'][:60]} {blank}\")\n",
    "\n",
    "# Count total lines\n",
    "import subprocess\n",
    "result = subprocess.run(['wc', '-l', output_file], capture_output=True, text=True)\n",
    "print(f\"\\nTotal lines in rebuilt corpus: {result.stdout.split()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Optional - Compress Output\n",
    "\n",
    "Compress the JSONL file to save space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "input_file = '/content/drive/MyDrive/AI and Poetry/Historical Embeddings/gutenberg_poetry_corpus_clean.jsonl'\n",
    "output_gz = input_file + '.gz'\n",
    "\n",
    "print(f\"Compressing {input_file}...\")\n",
    "\n",
    "with open(input_file, 'rb') as f_in:\n",
    "    with gzip.open(output_gz, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Check sizes\n",
    "import os\n",
    "orig_size = os.path.getsize(input_file) / (1024**2)\n",
    "comp_size = os.path.getsize(output_gz) / (1024**2)\n",
    "\n",
    "print(f\"\\nOriginal: {orig_size:.1f} MB\")\n",
    "print(f\"Compressed: {comp_size:.1f} MB\")\n",
    "print(f\"Compression: {(1 - comp_size/orig_size)*100:.1f}%\")\n",
    "print(f\"\\n✓ Saved to: {output_gz}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
