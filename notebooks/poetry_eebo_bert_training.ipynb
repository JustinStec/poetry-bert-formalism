{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poetry-EEBO-BERT Training: Layer 2 Architecture\n",
    "\n",
    "**This notebook trains the proper Layer 2 model:**\n",
    "- \u2705 Starts FROM EEBO-BERT (Layer 1)\n",
    "- \u2705 Fine-tunes on 17.7M lines of poetry\n",
    "- \u2705 Creates Poetry-EEBO-BERT (Layer 1 \u2192 Layer 2)\n",
    "\n",
    "## Architecture Path:\n",
    "```\n",
    "bert-base-uncased\n",
    "    \u2193 Fine-tune on EEBO 1595-1700\n",
    "EEBO-BERT (Layer 1) \u2713 COMPLETE\n",
    "    \u2193 Fine-tune on poetry corpus (THIS NOTEBOOK)\n",
    "Poetry-EEBO-BERT (Layer 2) \u23f3 TRAINING\n",
    "```\n",
    "\n",
    "## Before you start:\n",
    "\n",
    "**Required files in Google Drive:**\n",
    "1. **EEBO-BERT model** (Layer 1): `MyDrive/EEBO_1595-1700/eebo_bert_finetuned/`\n",
    "2. **Poetry database**: `MyDrive/poetry_unified.db` (or build it below)\n",
    "\n",
    "**Estimated time:** 6-8 hours on Colab A100 GPU\n",
    "\n",
    "**Cost:** Free tier sufficient (A100 available in free Colab)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup - Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate tqdm -q\n",
    "print(\"\u2713 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"\u2713 Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check GPU\n",
    "\n",
    "**IMPORTANT:** Request an **A100 GPU** for fastest training:\n",
    "1. Runtime \u2192 Change runtime type\n",
    "2. Hardware accelerator: GPU\n",
    "3. GPU type: A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    if 'A100' in torch.cuda.get_device_name(0):\n",
    "        print(\"\\n\u2713 A100 detected! Training will be FAST (~6 hours)\")\n",
    "    elif 'T4' in torch.cuda.get_device_name(0):\n",
    "        print(\"\\n\u26a0 T4 detected. Training will be slower (~12-14 hours)\")\n",
    "        print(\"  Consider requesting A100 in Runtime settings\")\n",
    "else:\n",
    "    print(\"\\n\u274c No GPU detected! Please enable GPU in Runtime settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify EEBO-BERT Model\n",
    "\n",
    "Check that Layer 1 (EEBO-BERT) exists in Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# EEBO-BERT path (Layer 1)\n",
    "EEBO_BERT_PATH = Path(\"/content/drive/MyDrive/AI and Poetry/EEBO_1595-1700/eebo_bert_finetuned\")\n",
    "\n",
    "print(\"Checking for EEBO-BERT model...\")\n",
    "if EEBO_BERT_PATH.exists():\n",
    "    config_file = EEBO_BERT_PATH / \"config.json\"\n",
    "    model_file = EEBO_BERT_PATH / \"pytorch_model.bin\"\n",
    "    \n",
    "    if config_file.exists() and model_file.exists():\n",
    "        size_mb = model_file.stat().st_size / (1024**2)\n",
    "        print(f\"\u2713 EEBO-BERT found: {size_mb:.1f} MB\")\n",
    "        print(f\"  Path: {EEBO_BERT_PATH}\")\n",
    "    else:\n",
    "        print(\"\u274c EEBO-BERT folder exists but missing model files!\")\n",
    "        print(\"  Expected: config.json and pytorch_model.bin\")\n",
    "else:\n",
    "    print(f\"\u274c EEBO-BERT not found at {EEBO_BERT_PATH}\")\n",
    "    print(\"  Please check Google Drive path!\")\n",
    "    print(\"  Expected: MyDrive/EEBO_1595-1700/eebo_bert_finetuned/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Check/Prepare Poetry Database\n",
    "\n",
    "Option A: Use existing `poetry_unified.db` from Google Drive\n",
    "\n",
    "Option B: Build it (see Step 5B below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Check for existing database\n",
    "DB_PATH_DRIVE = Path(\"/content/drive/MyDrive/AI and Poetry/poetry_unified.db\")\n",
    "DB_PATH_LOCAL = Path(\"/content/poetry_unified.db\")\n",
    "\n",
    "print(\"Checking for poetry database...\")\n",
    "\n",
    "if DB_PATH_DRIVE.exists():\n",
    "    size_gb = DB_PATH_DRIVE.stat().st_size / (1024**3)\n",
    "    print(f\"\u2713 Found in Google Drive: {size_gb:.2f} GB\")\n",
    "    \n",
    "    # Copy to local for faster access\n",
    "    print(\"  Copying to local storage for faster access...\")\n",
    "    !cp \"{DB_PATH_DRIVE}\" \"{DB_PATH_LOCAL}\"\n",
    "    print(\"  \u2713 Copied to local\")\n",
    "    \n",
    "    # Check line count\n",
    "    conn = sqlite3.connect(DB_PATH_LOCAL)\n",
    "    cursor = conn.cursor()\n",
    "    line_count = cursor.execute(\"SELECT COUNT(*) FROM lines WHERE is_blank = 0\").fetchone()[0]\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"  \u2713 Database contains {line_count:,} non-blank lines\")\n",
    "    \n",
    "else:\n",
    "    print(\"\u274c poetry_unified.db not found in Google Drive\")\n",
    "    print(\"  Expected: MyDrive/poetry_unified.db\")\n",
    "    print(\"  Run Step 5B below to build it, or upload it manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5B (Optional): Build Poetry Database\n",
    "\n",
    "**Only run this if you don't have `poetry_unified.db` in Google Drive**\n",
    "\n",
    "Required JSONL files in `MyDrive/AI and Poetry/Data/Databases/poetry_corpus/`:\n",
    "- `shakespeare_complete_works.jsonl`\n",
    "- `gutenberg_reconstructed.jsonl`\n",
    "- `core_poets_complete.jsonl`\n",
    "- `poetrydb.jsonl`\n",
    "\n",
    "This will take 2-4 hours. Skip if you already have the database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run ONLY if you need to build the database:\n",
    "\n",
    "# !pip install tqdm\n",
    "# \n",
    "# # Run the database builder from the previous notebook\n",
    "# # (Copy the UnifiedDatabaseBuilder class and build code here)\n",
    "# print(\"See poetry_bert_training_full_pipeline.ipynb for database building code\")\n",
    "# print(\"Or upload poetry_unified.db directly to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export Training Corpus from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "CORPUS_OUTPUT = \"/content/poetry_training_corpus.txt\"\n",
    "\n",
    "print(\"Exporting training corpus from database...\")\n",
    "print(f\"Database: {DB_PATH_LOCAL}\")\n",
    "print(f\"Output: {CORPUS_OUTPUT}\")\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(DB_PATH_LOCAL)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get total line count\n",
    "total_lines = cursor.execute(\"SELECT COUNT(*) FROM lines WHERE is_blank = 0\").fetchone()[0]\n",
    "print(f\"\\nExporting {total_lines:,} non-blank lines...\")\n",
    "\n",
    "# Export lines\n",
    "with open(CORPUS_OUTPUT, 'w', encoding='utf-8') as f:\n",
    "    cursor.execute(\"SELECT line_text FROM lines WHERE is_blank = 0 ORDER BY line_id\")\n",
    "    \n",
    "    batch_size = 10000\n",
    "    pbar = tqdm(total=total_lines, desc=\"Exporting\")\n",
    "    \n",
    "    while True:\n",
    "        rows = cursor.fetchmany(batch_size)\n",
    "        if not rows:\n",
    "            break\n",
    "        \n",
    "        for (line_text,) in rows:\n",
    "            f.write(line_text + '\\n')\n",
    "        \n",
    "        pbar.update(len(rows))\n",
    "    \n",
    "    pbar.close()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "size_gb = os.path.getsize(CORPUS_OUTPUT) / (1024**3)\n",
    "print(f\"\\n\u2713 Corpus exported: {size_gb:.2f} GB\")\n",
    "print(f\"\u2713 {total_lines:,} lines ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Load EEBO-BERT (Layer 1) as Starting Point\n",
    "\n",
    "**Critical:** We load EEBO-BERT, NOT bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING EEBO-BERT (LAYER 1) AS STARTING POINT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Path: {EEBO_BERT_PATH}\")\n",
    "print()\n",
    "\n",
    "# Load tokenizer and model from EEBO-BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(str(EEBO_BERT_PATH))\n",
    "model = BertForMaskedLM.from_pretrained(str(EEBO_BERT_PATH))\n",
    "\n",
    "print(f\"\u2713 EEBO-BERT loaded: {model.num_parameters():,} parameters\")\n",
    "print(f\"\u2713 Tokenizer vocab size: {len(tokenizer):,}\")\n",
    "print()\n",
    "print(\"This model will now be fine-tuned on poetry (Layer 1 \u2192 Layer 2)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Load and Tokenize Poetry Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Load corpus\n",
    "print(f\"Loading corpus from {CORPUS_OUTPUT}...\")\n",
    "dataset = load_dataset('text', data_files={'train': CORPUS_OUTPUT}, split='train')\n",
    "print(f\"\u2713 Loaded {len(dataset):,} lines\")\n",
    "\n",
    "# Tokenize\n",
    "print(\"\\nTokenizing corpus...\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "print(\"\u2713 Tokenization complete\")\n",
    "print(f\"  Total examples: {len(tokenized_dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Configure Training\n",
    "\n",
    "**Training parameters optimized for poetry fine-tuning:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Training configuration\n",
    "BATCH_SIZE = 8  # Good for A100\n",
    "NUM_EPOCHS = 3  # Standard for fine-tuning\n",
    "LEARNING_RATE = 5e-5  # Standard BERT fine-tuning rate\n",
    "SAVE_STEPS = 1000\n",
    "LOGGING_STEPS = 100\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Save every: {SAVE_STEPS} steps\")\n",
    "print(f\"  Log every: {LOGGING_STEPS} steps\")\n",
    "\n",
    "# Calculate total steps\n",
    "total_steps = len(tokenized_dataset) // BATCH_SIZE * NUM_EPOCHS\n",
    "print(f\"\\nTotal training steps: {total_steps:,}\")\n",
    "print(f\"Estimated time on A100: ~6 hours\")\n",
    "print(f\"Estimated time on T4: ~12-14 hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./poetry_eebo_bert_checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,  # Keep only 2 most recent checkpoints\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # Mixed precision for speed\n",
    "    logging_dir='./logs',\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"\u2713 Training setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Start Training (6-8 hours)\n",
    "\n",
    "\u26a0\ufe0f **This will take 6-8 hours on A100 GPU**\n",
    "\n",
    "You can close the tab - training will continue in the background.\n",
    "\n",
    "**The model will be saved automatically every 1000 steps to Google Drive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING POETRY-EEBO-BERT TRAINING (LAYER 1 \u2192 LAYER 2)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Estimated duration: 6-8 hours on A100\")\n",
    "print(\"You can close this tab - training will continue\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(f\"\u2713 TRAINING COMPLETE!\")\n",
    "print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to local Colab storage\n",
    "output_dir = \"./poetry_eebo_bert_trained\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"\u2713 Model saved to local: {output_dir}\")\n",
    "\n",
    "# Save to Google Drive (IMPORTANT!)\n",
    "drive_output = \"/content/drive/MyDrive/AI and Poetry/poetry_eebo_bert_trained\"\n",
    "!mkdir -p \"{drive_output}\"\n",
    "!cp -r {output_dir}/* \"{drive_output}/\"\n",
    "print(f\"\u2713 Model saved to Google Drive: {drive_output}\")\n",
    "print()\n",
    "print(\"IMPORTANT: Download this model to your local machine!\")\n",
    "print(f\"  Location: MyDrive/poetry_eebo_bert_trained/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Test the Model\n",
    "\n",
    "Quick sanity check that the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the trained model\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(\"Testing Poetry-EEBO-BERT (Layer 2)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test on Shakespeare (historical + poetry)\n",
    "test_text = \"Shall I compare thee to a [MASK] day?\"\n",
    "print(f\"\\nTest 1: '{test_text}'\")\n",
    "predictions = fill_mask(test_text)\n",
    "for i, pred in enumerate(predictions[:5], 1):\n",
    "    print(f\"  {i}. {pred['token_str']}: {pred['score']:.4f}\")\n",
    "\n",
    "# Test on iambic line\n",
    "test_text2 = \"The [MASK] of spirit in a waste of shame\"\n",
    "print(f\"\\nTest 2: '{test_text2}'\")\n",
    "predictions2 = fill_mask(test_text2)\n",
    "for i, pred in enumerate(predictions2[:5], 1):\n",
    "    print(f\"  {i}. {pred['token_str']}: {pred['score']:.4f}\")\n",
    "\n",
    "# Test on rhyming couplet\n",
    "test_text3 = \"So long as men can breathe or eyes can [MASK]\"\n",
    "print(f\"\\nTest 3: '{test_text3}'\")\n",
    "predictions3 = fill_mask(test_text3)\n",
    "for i, pred in enumerate(predictions3[:5], 1):\n",
    "    print(f\"  {i}. {pred['token_str']}: {pred['score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2713 Model is working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Create Documentation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create README for the model\n",
    "readme_content = f\"\"\"# Poetry-EEBO-BERT (Layer 2)\n",
    "\n",
    "**Trained:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Architecture Path:\n",
    "\n",
    "```\n",
    "bert-base-uncased (110M parameters)\n",
    "    \u2193 Fine-tune on EEBO 1595-1700\n",
    "EEBO-BERT (Layer 1 - Historical Semantics)\n",
    "    \u2193 Fine-tune on 17.7M poetry lines (THIS MODEL)\n",
    "Poetry-EEBO-BERT (Layer 2 - Poetry + Historical)\n",
    "```\n",
    "\n",
    "## Training Details:\n",
    "\n",
    "- **Base model:** EEBO-BERT (Layer 1)\n",
    "- **Training corpus:** 17.7M lines of poetry\n",
    "- **Sources:** Shakespeare, Gutenberg, Core 27 Poets, PoetryDB\n",
    "- **Epochs:** {NUM_EPOCHS}\n",
    "- **Batch size:** {BATCH_SIZE}\n",
    "- **Learning rate:** {LEARNING_RATE}\n",
    "- **Max length:** {MAX_LENGTH}\n",
    "- **Total steps:** {total_steps:,}\n",
    "- **Training time:** {total_time/3600:.2f} hours\n",
    "- **GPU:** {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\n",
    "\n",
    "## Purpose:\n",
    "\n",
    "This model captures both:\n",
    "1. **Historical semantics** (from EEBO-BERT training on 1595-1700 texts)\n",
    "2. **Poetic conventions** (from poetry corpus training)\n",
    "\n",
    "## Usage:\n",
    "\n",
    "```python\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./poetry_eebo_bert_trained')\n",
    "model = BertModel.from_pretrained('./poetry_eebo_bert_trained')\n",
    "```\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "1. Run Layer 3 analysis with prosodic conditioning\n",
    "2. Compare with Base BERT, EEBO-BERT, and Poetry-BERT\n",
    "3. Analyze Shakespeare sonnets trajectory tortuosity\n",
    "\n",
    "## Citation:\n",
    "\n",
    "```bibtex\n",
    "@unpublished{{stecher2025poetry_eebo_bert,\n",
    "  title={{Poetry-EEBO-BERT: A Layered Architecture for Historical Poetry Analysis}},\n",
    "  author={{Stecher, Justin}},\n",
    "  year={{2025}},\n",
    "  note={{Layer 2 of three-layer BERT architecture}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Save README\n",
    "readme_path_local = f\"{output_dir}/README.md\"\n",
    "readme_path_drive = f\"{drive_output}/README.md\"\n",
    "\n",
    "with open(readme_path_local, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "with open(readme_path_drive, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"\u2713 README.md created in model directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "\u2705 **Loaded EEBO-BERT (Layer 1) as starting point**\n",
    "\n",
    "\u2705 **Fine-tuned on 17.7M lines of poetry**\n",
    "\n",
    "\u2705 **Saved Poetry-EEBO-BERT (Layer 2) to Google Drive**\n",
    "\n",
    "\u2705 **Model tested and working**\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "1. **Download the model** from Google Drive:\n",
    "   - `MyDrive/poetry_eebo_bert_trained/`\n",
    "\n",
    "2. **Run Layer 3 analysis** on local machine:\n",
    "   ```bash\n",
    "   python scripts/layer3_bert_prosody.py --model poetry_eebo\n",
    "   ```\n",
    "\n",
    "3. **Compare all models**:\n",
    "   - Base BERT (baseline)\n",
    "   - EEBO-BERT (Layer 1 - historical)\n",
    "   - Poetry-BERT (Layer 2 - independent poetry path)\n",
    "   - **Poetry-EEBO-BERT (Layer 2 - proper layered path)** \u2190 NEW!\n",
    "\n",
    "4. **Analyze results** in `notebooks/complete_layered_analysis.ipynb`\n",
    "\n",
    "5. **Start writing Paper 1** (DH venue) with complete architecture!\n",
    "\n",
    "---\n",
    "\n",
    "**Model location:** `MyDrive/poetry_eebo_bert_trained/`\n",
    "\n",
    "**Training completed:** See timestamp above\n",
    "\n",
    "**Ready for:** Shakespeare sonnets trajectory tortuosity analysis with Layer 3 prosodic conditioning"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}