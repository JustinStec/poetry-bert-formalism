{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JustinStec/poetry-bert-formalism/blob/main/notebooks/hierarchical_bert_training_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vFX9Jif0SaO"
      },
      "source": [
        "# Hierarchical Multi-Objective Poetry-EEBO-BERT Training\n",
        "\n",
        "Train BERT with hierarchical losses on Google Colab:\n",
        "- **0.5** √ó MLM (token level)\n",
        "- **0.2** √ó Line contrastive\n",
        "- **0.2** √ó Quatrain contrastive\n",
        "- **0.1** √ó Sonnet contrastive\n",
        "\n",
        "## Requirements\n",
        "- Google Colab with GPU runtime (T4/A100)\n",
        "- Google Drive mounted with EEBO-BERT checkpoint\n",
        "- Training data (Shakespeare sonnets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scjrDbCR0SaP"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly1qLPXi0SaP"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BerAgcAE0SaQ"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers==4.36.0 torch==2.1.0 datasets==2.15.0 tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with HuggingFace\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token='YOUR_HF_TOKEN_HERE')\n",
        "print(\"‚úì Authenticated with HuggingFace\")"
      ],
      "metadata": {
        "id": "GW1ljQNw0SaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAUztKqH0SaQ"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeCvnItf0SaQ"
      },
      "source": [
        "## 2. Clone Repository and Upload Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_IKRqrV0SaQ"
      },
      "outputs": [],
      "source": [
        "### Upload Required Files\n",
        "\n",
        "Use the file upload button in Colab's left sidebar (üìÅ icon) to upload:\n",
        "\n",
        "**Training Modules:**\n",
        "1. `training/hierarchical_dataset.py`\n",
        "2. `training/hierarchical_losses.py`\n",
        "3. `training/hierarchical_trainer.py`\n",
        "\n",
        "**Training Data:**\n",
        "4. `Data/eebo_sonnets_hierarchical_train.jsonl`\n",
        "5. `Data/eebo_sonnets_hierarchical_val.jsonl`\n",
        "\n",
        "**Where to find these files:**\n",
        "All files are in `/Users/justin/Repos/AI Project/`\n",
        "\n",
        "**Upload location in Colab:**\n",
        "- Upload the 3 `.py` files to `/content/training/`\n",
        "- Upload the 2 `.jsonl` files to `/content/Data/`\n",
        "\n",
        "Or just drag and drop all 5 files to the Files panel and run:\n",
        "```python\n",
        "!mkdir -p training Data\n",
        "!mv hierarchical_*.py training/\n",
        "!mv *.jsonl Data/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbe2_5fB0SaQ"
      },
      "source": [
        "### Upload Training Module Files\n",
        "\n",
        "Upload these files from your local machine:\n",
        "1. `training/hierarchical_dataset.py`\n",
        "2. `training/hierarchical_losses.py`\n",
        "3. `training/hierarchical_trainer.py`\n",
        "4. `Data/eebo_sonnets_hierarchical_train.jsonl`\n",
        "5. `Data/eebo_sonnets_hierarchical_val.jsonl`\n",
        "\n",
        "Use the file upload button in Colab's left sidebar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDE8VC0SaQ"
      },
      "source": [
        "## 3. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXv_6vAF0SaR"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    # Model - using HuggingFace hosted model\n",
        "    'base_model': 'jts3et/eebo-bert',\n",
        "    'hf_token': 'YOUR_HF_TOKEN_HERE',  # Your HuggingFace token\n",
        "\n",
        "    # Data paths (will be uploaded to Colab)\n",
        "    'train_data': 'Data/eebo_sonnets_hierarchical_train.jsonl',\n",
        "    'val_data': 'Data/eebo_sonnets_hierarchical_val.jsonl',\n",
        "\n",
        "    # Output\n",
        "    'output_dir': 'models/poetry_eebo_hierarchical_bert',\n",
        "    'save_to_drive': '/content/drive/MyDrive/AI and Poetry/poetry_eebo_hierarchical_bert',\n",
        "\n",
        "    # Training hyperparameters (optimized for GPU)\n",
        "    'batch_size': 8,  # Good for A100/T4\n",
        "    'num_epochs': 10,\n",
        "    'learning_rate': 2e-5,\n",
        "    'warmup_steps': 100,\n",
        "    'max_length': 128,\n",
        "\n",
        "    # Loss weights\n",
        "    'mlm_weight': 0.5,\n",
        "    'line_weight': 0.2,\n",
        "    'quatrain_weight': 0.2,\n",
        "    'sonnet_weight': 0.1,\n",
        "    'temperature': 0.07,\n",
        "\n",
        "    # Other\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    if key == 'hf_token':\n",
        "        print(f\"  {key}: {'*' * 20} (hidden)\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjiHgrYV0SaR"
      },
      "source": [
        "## 4. Import Training Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSvDsNM10SaR"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, TrainingArguments\n",
        "from training.hierarchical_dataset import HierarchicalPoetryDataset, collate_hierarchical\n",
        "from training.hierarchical_losses import HierarchicalLoss\n",
        "from training.hierarchical_trainer import HierarchicalBertModel, HierarchicalTrainer\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6UVu4XE0SaR"
      },
      "source": [
        "## 5. Load Data and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D20wR3i0SaR"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "torch.manual_seed(CONFIG['seed'])\n",
        "\n",
        "# Load tokenizer\n",
        "print(f\"Loading tokenizer from {CONFIG['base_model']}...\")\n",
        "tokenizer = BertTokenizer.from_pretrained(CONFIG['base_model'])\n",
        "print(\"‚úì Tokenizer loaded\")\n",
        "\n",
        "# Load datasets\n",
        "print(f\"\\nLoading training data...\")\n",
        "train_dataset = HierarchicalPoetryDataset(\n",
        "    data_path=CONFIG['train_data'],\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=CONFIG['max_length'],\n",
        "    mlm_probability=0.15\n",
        ")\n",
        "\n",
        "print(f\"Loading validation data...\")\n",
        "val_dataset = HierarchicalPoetryDataset(\n",
        "    data_path=CONFIG['val_data'],\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=CONFIG['max_length'],\n",
        "    mlm_probability=0.15\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  Train: {len(train_dataset)} sonnets\")\n",
        "print(f\"  Val: {len(val_dataset)} sonnets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKJUdWlr0SaR"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "print(f\"\\nInitializing model from {CONFIG['base_model']}...\")\n",
        "model = HierarchicalBertModel(base_model_path=CONFIG['base_model'])\n",
        "print(\"‚úì Model initialized\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nModel parameters:\")\n",
        "print(f\"  Total: {total_params:,}\")\n",
        "print(f\"  Trainable: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZdW20nE0SaR"
      },
      "source": [
        "## 6. Setup Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0JXXopO0SaR"
      },
      "outputs": [],
      "source": [
        "# Initialize loss function\n",
        "loss_fn = HierarchicalLoss(\n",
        "    temperature=CONFIG['temperature'],\n",
        "    mlm_weight=CONFIG['mlm_weight'],\n",
        "    line_weight=CONFIG['line_weight'],\n",
        "    quatrain_weight=CONFIG['quatrain_weight'],\n",
        "    sonnet_weight=CONFIG['sonnet_weight']\n",
        ")\n",
        "\n",
        "print(\"Loss configuration:\")\n",
        "print(f\"  MLM weight: {CONFIG['mlm_weight']}\")\n",
        "print(f\"  Line weight: {CONFIG['line_weight']}\")\n",
        "print(f\"  Quatrain weight: {CONFIG['quatrain_weight']}\")\n",
        "print(f\"  Sonnet weight: {CONFIG['sonnet_weight']}\")\n",
        "print(f\"  Temperature: {CONFIG['temperature']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M4Z9xKB0SaR"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=CONFIG['output_dir'],\n",
        "    num_train_epochs=CONFIG['num_epochs'],\n",
        "    per_device_train_batch_size=CONFIG['batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    warmup_steps=CONFIG['warmup_steps'],\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=True,  # Mixed precision training\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    seed=CONFIG['seed']\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = HierarchicalTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=collate_hierarchical,\n",
        "    loss_fn=loss_fn\n",
        ")\n",
        "\n",
        "print(\"‚úì Trainer initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo_vIlln0SaR"
      },
      "source": [
        "## 7. Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkhKnPoU0SaR"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
        "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"Device: {training_args.device}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5-5LfMn0SaS"
      },
      "source": [
        "## 8. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lsan_cLy0SaS"
      },
      "outputs": [],
      "source": [
        "# Save final model locally\n",
        "final_model_path = f\"{CONFIG['output_dir']}/final\"\n",
        "trainer.save_model(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "print(f\"‚úì Model saved locally to {final_model_path}\")\n",
        "\n",
        "# Copy to Google Drive\n",
        "import shutil\n",
        "if CONFIG['save_to_drive']:\n",
        "    print(f\"\\nCopying model to Google Drive: {CONFIG['save_to_drive']}\")\n",
        "    shutil.copytree(final_model_path, CONFIG['save_to_drive'], dirs_exist_ok=True)\n",
        "    print(\"‚úì Model saved to Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QkUSJFG0SaS"
      },
      "source": [
        "## 9. View Training Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhBrbOg90SaS"
      },
      "outputs": [],
      "source": [
        "# Load tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {CONFIG['output_dir']}/logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6pkLYyc0SaS"
      },
      "outputs": [],
      "source": [
        "# Print loss history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if trainer.loss_history['total']:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    axes[0, 0].plot(trainer.loss_history['total'])\n",
        "    axes[0, 0].set_title('Total Loss')\n",
        "    axes[0, 0].set_xlabel('Step')\n",
        "\n",
        "    axes[0, 1].plot(trainer.loss_history['mlm'], label='MLM', color='blue')\n",
        "    axes[0, 1].set_title('MLM Loss')\n",
        "    axes[0, 1].set_xlabel('Step')\n",
        "\n",
        "    axes[1, 0].plot(trainer.loss_history['line'], label='Line', color='green')\n",
        "    axes[1, 0].plot(trainer.loss_history['quatrain'], label='Quatrain', color='orange')\n",
        "    axes[1, 0].set_title('Line & Quatrain Contrastive Loss')\n",
        "    axes[1, 0].set_xlabel('Step')\n",
        "    axes[1, 0].legend()\n",
        "\n",
        "    axes[1, 1].plot(trainer.loss_history['sonnet'], label='Sonnet', color='red')\n",
        "    axes[1, 1].set_title('Sonnet Contrastive Loss')\n",
        "    axes[1, 1].set_xlabel('Step')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{CONFIG['output_dir']}/loss_curves.png\", dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nFinal losses (last 10 steps average):\")\n",
        "    print(f\"  Total: {sum(trainer.loss_history['total'][-10:]) / 10:.4f}\")\n",
        "    print(f\"  MLM: {sum(trainer.loss_history['mlm'][-10:]) / 10:.4f}\")\n",
        "    print(f\"  Line: {sum(trainer.loss_history['line'][-10:]) / 10:.4f}\")\n",
        "    print(f\"  Quatrain: {sum(trainer.loss_history['quatrain'][-10:]) / 10:.4f}\")\n",
        "    print(f\"  Sonnet: {sum(trainer.loss_history['sonnet'][-10:]) / 10:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU_EtugZ0SaS"
      },
      "source": [
        "## 10. Test Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi_mSpuD0SaS"
      },
      "outputs": [],
      "source": [
        "# Quick test: encode a sonnet line\n",
        "test_line = \"Shall I compare thee to a summer's day?\"\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    inputs = tokenizer(test_line, return_tensors='pt').to(training_args.device)\n",
        "    outputs = model.bert(**inputs)\n",
        "    embedding = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "print(f\"Test line: {test_line}\")\n",
        "print(f\"Embedding shape: {embedding.shape}\")\n",
        "print(f\"Embedding norm: {embedding.norm().item():.4f}\")\n",
        "print(\"\\n‚úì Model is working correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_k2d_z20SaS"
      },
      "source": [
        "## Training Complete!\n",
        "\n",
        "Your hierarchical Poetry-EEBO-BERT model is now trained and saved to:\n",
        "- Local: `models/poetry_eebo_hierarchical_bert/final`\n",
        "- Google Drive: (path specified in CONFIG)\n",
        "\n",
        "Next steps:\n",
        "1. Download model from Google Drive to local machine\n",
        "2. Run validation scripts to compare with baseline models\n",
        "3. Analyze trajectory tortuosity on Shakespeare's sonnets\n",
        "4. Generate results for Paper 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}