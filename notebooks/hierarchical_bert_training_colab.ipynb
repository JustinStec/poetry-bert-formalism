{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Multi-Objective Poetry-EEBO-BERT Training\n",
    "\n",
    "Train BERT with hierarchical losses on Google Colab:\n",
    "- **0.5** √ó MLM (token level)\n",
    "- **0.2** √ó Line contrastive\n",
    "- **0.2** √ó Quatrain contrastive\n",
    "- **0.1** √ó Sonnet contrastive\n",
    "\n",
    "## Requirements\n",
    "- Google Colab with GPU runtime (T4/A100)\n",
    "- Google Drive mounted with EEBO-BERT checkpoint\n",
    "- Training data (Shakespeare sonnets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers==4.36.0 torch==2.1.0 datasets==2.15.0 tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Authenticate with HuggingFace\n!pip install -q huggingface_hub\n\nfrom huggingface_hub import login\nlogin(token='YOUR_HF_TOKEN_HERE')\nprint(\"‚úì Authenticated with HuggingFace\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository and Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Upload Required Files\n\nUse the file upload button in Colab's left sidebar (üìÅ icon) to upload:\n\n**Training Modules:**\n1. `training/hierarchical_dataset.py`\n2. `training/hierarchical_losses.py`\n3. `training/hierarchical_trainer.py`\n\n**Training Data:**\n4. `Data/eebo_sonnets_hierarchical_train.jsonl`\n5. `Data/eebo_sonnets_hierarchical_val.jsonl`\n\n**Where to find these files:**\nAll files are in `/Users/justin/Repos/AI Project/`\n\n**Upload location in Colab:**\n- Upload the 3 `.py` files to `/content/training/`\n- Upload the 2 `.jsonl` files to `/content/Data/`\n\nOr just drag and drop all 5 files to the Files panel and run:\n```python\n!mkdir -p training Data\n!mv hierarchical_*.py training/\n!mv *.jsonl Data/\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Training Module Files\n",
    "\n",
    "Upload these files from your local machine:\n",
    "1. `training/hierarchical_dataset.py`\n",
    "2. `training/hierarchical_losses.py`\n",
    "3. `training/hierarchical_trainer.py`\n",
    "4. `Data/eebo_sonnets_hierarchical_train.jsonl`\n",
    "5. `Data/eebo_sonnets_hierarchical_val.jsonl`\n",
    "\n",
    "Use the file upload button in Colab's left sidebar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nCONFIG = {\n    # Model - using HuggingFace hosted model\n    'base_model': 'jts3et/eebo-bert',\n    'hf_token': 'YOUR_HF_TOKEN_HERE',  # Your HuggingFace token\n    \n    # Data paths (will be uploaded to Colab)\n    'train_data': 'Data/eebo_sonnets_hierarchical_train.jsonl',\n    'val_data': 'Data/eebo_sonnets_hierarchical_val.jsonl',\n    \n    # Output\n    'output_dir': 'models/poetry_eebo_hierarchical_bert',\n    'save_to_drive': '/content/drive/MyDrive/AI and Poetry/poetry_eebo_hierarchical_bert',\n    \n    # Training hyperparameters (optimized for GPU)\n    'batch_size': 8,  # Good for A100/T4\n    'num_epochs': 10,\n    'learning_rate': 2e-5,\n    'warmup_steps': 100,\n    'max_length': 128,\n    \n    # Loss weights\n    'mlm_weight': 0.5,\n    'line_weight': 0.2,\n    'quatrain_weight': 0.2,\n    'sonnet_weight': 0.1,\n    'temperature': 0.07,\n    \n    # Other\n    'seed': 42\n}\n\nprint(\"Configuration:\")\nfor key, value in CONFIG.items():\n    if key == 'hf_token':\n        print(f\"  {key}: {'*' * 20} (hidden)\")\n    else:\n        print(f\"  {key}: {value}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import Training Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, TrainingArguments\n",
    "from training.hierarchical_dataset import HierarchicalPoetryDataset, collate_hierarchical\n",
    "from training.hierarchical_losses import HierarchicalLoss\n",
    "from training.hierarchical_trainer import HierarchicalBertModel, HierarchicalTrainer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer from {CONFIG['base_model']}...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG['base_model'])\n",
    "print(\"‚úì Tokenizer loaded\")\n",
    "\n",
    "# Load datasets\n",
    "print(f\"\\nLoading training data...\")\n",
    "train_dataset = HierarchicalPoetryDataset(\n",
    "    data_path=CONFIG['train_data'],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "print(f\"Loading validation data...\")\n",
    "val_dataset = HierarchicalPoetryDataset(\n",
    "    data_path=CONFIG['val_data'],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} sonnets\")\n",
    "print(f\"  Val: {len(val_dataset)} sonnets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(f\"\\nInitializing model from {CONFIG['base_model']}...\")\n",
    "model = HierarchicalBertModel(base_model_path=CONFIG['base_model'])\n",
    "print(\"‚úì Model initialized\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss function\n",
    "loss_fn = HierarchicalLoss(\n",
    "    temperature=CONFIG['temperature'],\n",
    "    mlm_weight=CONFIG['mlm_weight'],\n",
    "    line_weight=CONFIG['line_weight'],\n",
    "    quatrain_weight=CONFIG['quatrain_weight'],\n",
    "    sonnet_weight=CONFIG['sonnet_weight']\n",
    ")\n",
    "\n",
    "print(\"Loss configuration:\")\n",
    "print(f\"  MLM weight: {CONFIG['mlm_weight']}\")\n",
    "print(f\"  Line weight: {CONFIG['line_weight']}\")\n",
    "print(f\"  Quatrain weight: {CONFIG['quatrain_weight']}\")\n",
    "print(f\"  Sonnet weight: {CONFIG['sonnet_weight']}\")\n",
    "print(f\"  Temperature: {CONFIG['temperature']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    num_train_epochs=CONFIG['num_epochs'],\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    warmup_steps=CONFIG['warmup_steps'],\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,  # Mixed precision training\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    seed=CONFIG['seed']\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = HierarchicalTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_hierarchical,\n",
    "    loss_fn=loss_fn\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Device: {training_args.device}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model locally\n",
    "final_model_path = f\"{CONFIG['output_dir']}/final\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"‚úì Model saved locally to {final_model_path}\")\n",
    "\n",
    "# Copy to Google Drive\n",
    "import shutil\n",
    "if CONFIG['save_to_drive']:\n",
    "    print(f\"\\nCopying model to Google Drive: {CONFIG['save_to_drive']}\")\n",
    "    shutil.copytree(final_model_path, CONFIG['save_to_drive'], dirs_exist_ok=True)\n",
    "    print(\"‚úì Model saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {CONFIG['output_dir']}/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print loss history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if trainer.loss_history['total']:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    axes[0, 0].plot(trainer.loss_history['total'])\n",
    "    axes[0, 0].set_title('Total Loss')\n",
    "    axes[0, 0].set_xlabel('Step')\n",
    "    \n",
    "    axes[0, 1].plot(trainer.loss_history['mlm'], label='MLM', color='blue')\n",
    "    axes[0, 1].set_title('MLM Loss')\n",
    "    axes[0, 1].set_xlabel('Step')\n",
    "    \n",
    "    axes[1, 0].plot(trainer.loss_history['line'], label='Line', color='green')\n",
    "    axes[1, 0].plot(trainer.loss_history['quatrain'], label='Quatrain', color='orange')\n",
    "    axes[1, 0].set_title('Line & Quatrain Contrastive Loss')\n",
    "    axes[1, 0].set_xlabel('Step')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    axes[1, 1].plot(trainer.loss_history['sonnet'], label='Sonnet', color='red')\n",
    "    axes[1, 1].set_title('Sonnet Contrastive Loss')\n",
    "    axes[1, 1].set_xlabel('Step')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/loss_curves.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFinal losses (last 10 steps average):\")\n",
    "    print(f\"  Total: {sum(trainer.loss_history['total'][-10:]) / 10:.4f}\")\n",
    "    print(f\"  MLM: {sum(trainer.loss_history['mlm'][-10:]) / 10:.4f}\")\n",
    "    print(f\"  Line: {sum(trainer.loss_history['line'][-10:]) / 10:.4f}\")\n",
    "    print(f\"  Quatrain: {sum(trainer.loss_history['quatrain'][-10:]) / 10:.4f}\")\n",
    "    print(f\"  Sonnet: {sum(trainer.loss_history['sonnet'][-10:]) / 10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: encode a sonnet line\n",
    "test_line = \"Shall I compare thee to a summer's day?\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_line, return_tensors='pt').to(training_args.device)\n",
    "    outputs = model.bert(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "print(f\"Test line: {test_line}\")\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding norm: {embedding.norm().item():.4f}\")\n",
    "print(\"\\n‚úì Model is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Complete!\n",
    "\n",
    "Your hierarchical Poetry-EEBO-BERT model is now trained and saved to:\n",
    "- Local: `models/poetry_eebo_hierarchical_bert/final`\n",
    "- Google Drive: (path specified in CONFIG)\n",
    "\n",
    "Next steps:\n",
    "1. Download model from Google Drive to local machine\n",
    "2. Run validation scripts to compare with baseline models\n",
    "3. Analyze trajectory tortuosity on Shakespeare's sonnets\n",
    "4. Generate results for Paper 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}