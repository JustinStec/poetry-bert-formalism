{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JustinStec/poetry-bert-formalism/blob/main/notebooks/hierarchical_bert_training_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b2PeFWc4sFI"
   },
   "source": [
    "# Hierarchical Multi-Objective Poetry-EEBO-BERT Training\n",
    "\n",
    "Train BERT with hierarchical losses on Google Colab:\n",
    "- **0.5** √ó MLM (token level)\n",
    "- **0.2** √ó Line contrastive\n",
    "- **0.2** √ó Quatrain contrastive\n",
    "- **0.1** √ó Sonnet contrastive\n",
    "\n",
    "## Requirements\n",
    "- Google Colab with GPU runtime (T4/A100)\n",
    "- Google Drive mounted with EEBO-BERT checkpoint\n",
    "- Training data (Shakespeare sonnets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0K8cY-B4sFI"
   },
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yrkfZyk4sFJ"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qd9xmV034sFJ"
   },
   "outputs": [],
   "source": "# Install dependencies (use Colab's existing PyTorch/NumPy, just upgrade transformers)\n!pip install -q transformers datasets tensorboard --upgrade"
  },
  {
   "cell_type": "code",
   "source": [
    "# Authenticate with HuggingFace\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token='YOUR_HF_TOKEN_HERE')\n",
    "print(\"‚úì Authenticated with HuggingFace\")"
   ],
   "metadata": {
    "id": "ji_jIdg74sFJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnjhEjQf4sFJ"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBlqMM324sFJ"
   },
   "source": [
    "## 2. Clone Repository and Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-P20d_5g4sFJ"
   },
   "outputs": [],
   "source": "# Clone the repository to get all training files\n!git clone https://github.com/JustinStec/poetry-bert-formalism.git\n%cd poetry-bert-formalism\n\n# Verify files exist\n!ls -lh training/hierarchical*.py\n!ls -lh Data/eebo_sonnets_hierarchical*.jsonl\n\nprint(\"\\n‚úì Repository cloned successfully!\")\nprint(\"‚úì Training modules and data files are ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SSQinQy4sFK"
   },
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYRs1HUW4sFK"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Model - using HuggingFace hosted model\n",
    "    'base_model': 'jts3et/eebo-bert',\n",
    "    'hf_token': 'YOUR_HF_TOKEN_HERE',  # Your HuggingFace token\n",
    "\n",
    "    # Data paths (will be uploaded to Colab)\n",
    "    'train_data': 'Data/eebo_sonnets_hierarchical_train.jsonl',\n",
    "    'val_data': 'Data/eebo_sonnets_hierarchical_val.jsonl',\n",
    "\n",
    "    # Output\n",
    "    'output_dir': 'models/poetry_eebo_hierarchical_bert',\n",
    "    'save_to_drive': '/content/drive/MyDrive/AI and Poetry/poetry_eebo_hierarchical_bert',\n",
    "\n",
    "    # Training hyperparameters (optimized for GPU)\n",
    "    'batch_size': 8,  # Good for A100/T4\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_steps': 100,\n",
    "    'max_length': 128,\n",
    "\n",
    "    # Loss weights\n",
    "    'mlm_weight': 0.5,\n",
    "    'line_weight': 0.2,\n",
    "    'quatrain_weight': 0.2,\n",
    "    'sonnet_weight': 0.1,\n",
    "    'temperature': 0.07,\n",
    "\n",
    "    # Other\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    if key == 'hf_token':\n",
    "        print(f\"  {key}: {'*' * 20} (hidden)\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtbhLxx14sFK"
   },
   "source": [
    "## 4. Import Training Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Je8IFJRM4sFK"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, TrainingArguments\n",
    "from training.hierarchical_dataset import HierarchicalPoetryDataset, collate_hierarchical\n",
    "from training.hierarchical_losses import HierarchicalLoss\n",
    "from training.hierarchical_trainer import HierarchicalBertModel, HierarchicalTrainer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rB2gD-j_4sFK"
   },
   "source": [
    "## 5. Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3yculwZ4sFK"
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer from {CONFIG['base_model']}...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG['base_model'])\n",
    "print(\"‚úì Tokenizer loaded\")\n",
    "\n",
    "# Load datasets\n",
    "print(f\"\\nLoading training data...\")\n",
    "train_dataset = HierarchicalPoetryDataset(\n",
    "    data_path=CONFIG['train_data'],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "print(f\"Loading validation data...\")\n",
    "val_dataset = HierarchicalPoetryDataset(\n",
    "    data_path=CONFIG['val_data'],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} sonnets\")\n",
    "print(f\"  Val: {len(val_dataset)} sonnets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e19c2H1H4sFK"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(f\"\\nInitializing model from {CONFIG['base_model']}...\")\n",
    "model = HierarchicalBertModel(base_model_path=CONFIG['base_model'])\n",
    "print(\"‚úì Model initialized\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ePTpe0v4sFK"
   },
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRSS2xFi4sFK"
   },
   "outputs": [],
   "source": [
    "# Initialize loss function\n",
    "loss_fn = HierarchicalLoss(\n",
    "    temperature=CONFIG['temperature'],\n",
    "    mlm_weight=CONFIG['mlm_weight'],\n",
    "    line_weight=CONFIG['line_weight'],\n",
    "    quatrain_weight=CONFIG['quatrain_weight'],\n",
    "    sonnet_weight=CONFIG['sonnet_weight']\n",
    ")\n",
    "\n",
    "print(\"Loss configuration:\")\n",
    "print(f\"  MLM weight: {CONFIG['mlm_weight']}\")\n",
    "print(f\"  Line weight: {CONFIG['line_weight']}\")\n",
    "print(f\"  Quatrain weight: {CONFIG['quatrain_weight']}\")\n",
    "print(f\"  Sonnet weight: {CONFIG['sonnet_weight']}\")\n",
    "print(f\"  Temperature: {CONFIG['temperature']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVUtIox14sFL"
   },
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    num_train_epochs=CONFIG['num_epochs'],\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    warmup_steps=CONFIG['warmup_steps'],\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,  # Mixed precision training\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    seed=CONFIG['seed']\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = HierarchicalTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_hierarchical,\n",
    "    loss_fn=loss_fn\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup ntfy notifications\n",
    "import requests\n",
    "\n",
    "NTFY_TOPIC = \"poetry-bert-training-justin\"  # Change this to your topic\n",
    "\n",
    "def send_ntfy(message, title=\"Poetry BERT Training\"):\n",
    "    \"\"\"Send notification via ntfy\"\"\"\n",
    "    try:\n",
    "        requests.post(\n",
    "            f\"https://ntfy.sh/{NTFY_TOPIC}\",\n",
    "            data=message.encode('utf-8'),\n",
    "            headers={\"Title\": title}\n",
    "        )\n",
    "        print(f\"üì± Sent notification: {title}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to send notification: {e}\")\n",
    "\n",
    "# Test notification\n",
    "send_ntfy(\"Setup complete! Ready to train üöÄ\", \"Setup Complete\")"
   ],
   "metadata": {
    "id": "7DjRxARl4sFL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Add notification callback for progress updates\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class NotifyCallback(TrainerCallback):\n",
    "    \"\"\"Send ntfy notifications at the end of each epoch\"\"\"\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch = int(state.epoch)\n",
    "        send_ntfy(f\"Epoch {epoch}/{CONFIG['num_epochs']} complete ‚úì\", \"Training Progress\")\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        send_ntfy(\"Training started! üöÄ\", \"Training Started\")\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        send_ntfy(\"Training completed successfully! ‚úÖ\", \"Training Complete\")\n",
    "\n",
    "# Add callback to trainer\n",
    "trainer.add_callback(NotifyCallback())\n",
    "print(\"‚úì Notification callback added\")"
   ],
   "metadata": {
    "id": "aQsnMBsb4sFL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Add checkpoint syncing callback\nimport os\nimport shutil\n\nclass CheckpointSyncCallback(TrainerCallback):\n    \"\"\"Sync checkpoints to Google Drive after each save\"\"\"\n    def on_save(self, args, state, control, **kwargs):\n        # Get the latest checkpoint directory\n        checkpoint_dir = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n        \n        if os.path.exists(checkpoint_dir) and CONFIG['save_to_drive']:\n            # Create Drive checkpoint path\n            drive_checkpoint = f\"{CONFIG['save_to_drive']}/checkpoint-{state.global_step}\"\n            \n            try:\n                # Copy checkpoint to Drive\n                print(f\"Syncing checkpoint to Google Drive: {drive_checkpoint}\")\n                shutil.copytree(checkpoint_dir, drive_checkpoint, dirs_exist_ok=True)\n                print(f\"‚úì Checkpoint {state.global_step} synced to Drive\")\n                \n                # Send notification\n                epoch = int(state.epoch)\n                send_ntfy(f\"Checkpoint saved to Drive (Epoch {epoch}, Step {state.global_step})\", \"Checkpoint Saved\")\n            except Exception as e:\n                print(f\"‚ö†Ô∏è Failed to sync checkpoint: {e}\")\n\n# Add checkpoint sync callback\ntrainer.add_callback(CheckpointSyncCallback())\nprint(\"‚úì Checkpoint sync callback added - checkpoints will be saved to Drive after each epoch\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9myl-qV4sFL"
   },
   "source": [
    "## 7. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqBUXdew4sFL"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Device: {training_args.device}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULSu1cA_4sFL"
   },
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4npbj8xk4sFL"
   },
   "outputs": [],
   "source": [
    "# Save final model locally\n",
    "final_model_path = f\"{CONFIG['output_dir']}/final\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"‚úì Model saved locally to {final_model_path}\")\n",
    "\n",
    "# Copy to Google Drive\n",
    "import shutil\n",
    "if CONFIG['save_to_drive']:\n",
    "    print(f\"\\nCopying model to Google Drive: {CONFIG['save_to_drive']}\")\n",
    "    shutil.copytree(final_model_path, CONFIG['save_to_drive'], dirs_exist_ok=True)\n",
    "    print(\"‚úì Model saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7-G11v94sFL"
   },
   "source": [
    "## 9. View Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrjvaY8_4sFL"
   },
   "outputs": [],
   "source": [
    "# Load tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {CONFIG['output_dir']}/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctKkz68T4sFL"
   },
   "outputs": [],
   "source": [
    "# Print loss history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if trainer.loss_history['total']:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    axes[0, 0].plot(trainer.loss_history['total'])\n",
    "    axes[0, 0].set_title('Total Loss')\n",
    "    axes[0, 0].set_xlabel('Step')\n",
    "\n",
    "    axes[0, 1].plot(trainer.loss_history['mlm'], label='MLM', color='blue')\n",
    "    axes[0, 1].set_title('MLM Loss')\n",
    "    axes[0, 1].set_xlabel('Step')\n",
    "\n",
    "    axes[1, 0].plot(trainer.loss_history['line'], label='Line', color='green')\n",
    "    axes[1, 0].plot(trainer.loss_history['quatrain'], label='Quatrain', color='orange')\n",
    "    axes[1, 0].set_title('Line & Quatrain Contrastive Loss')\n",
    "    axes[1, 0].set_xlabel('Step')\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    axes[1, 1].plot(trainer.loss_history['sonnet'], label='Sonnet', color='red')\n",
    "    axes[1, 1].set_title('Sonnet Contrastive Loss')\n",
    "    axes[1, 1].set_xlabel('Step')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/loss_curves.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nFinal losses (last 10 steps average):\")\n",
    "    print(f\"  Total: {sum(trainer.loss_history['total'][-10:]) / 10:.4f}\")\n",
    "    print(f\"  MLM: {sum(trainer.loss_history['mlm'][-10:]) / 10:.4f}\")\n",
    "    print(f\"  Line: {sum(trainer.loss_history['line'][-10:]) / 10:.4f}\")\n",
    "    print(f\"  Quatrain: {sum(trainer.loss_history['quatrain'][-10:]) / 10:.4f}\")\n",
    "    print(f\"  Sonnet: {sum(trainer.loss_history['sonnet'][-10:]) / 10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uotL8lGO4sFL"
   },
   "source": [
    "## 10. Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q72GwzJ74sFL"
   },
   "outputs": [],
   "source": [
    "# Quick test: encode a sonnet line\n",
    "test_line = \"Shall I compare thee to a summer's day?\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_line, return_tensors='pt').to(training_args.device)\n",
    "    outputs = model.bert(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "print(f\"Test line: {test_line}\")\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding norm: {embedding.norm().item():.4f}\")\n",
    "print(\"\\n‚úì Model is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW9u8A0L4sFL"
   },
   "source": [
    "## Training Complete!\n",
    "\n",
    "Your hierarchical Poetry-EEBO-BERT model is now trained and saved to:\n",
    "- Local: `models/poetry_eebo_hierarchical_bert/final`\n",
    "- Google Drive: (path specified in CONFIG)\n",
    "\n",
    "Next steps:\n",
    "1. Download model from Google Drive to local machine\n",
    "2. Run validation scripts to compare with baseline models\n",
    "3. Analyze trajectory tortuosity on Shakespeare's sonnets\n",
    "4. Generate results for Paper 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "machine_shape": "hm",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}