{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Gutenberg Poetry Corpus to Google Drive\n",
    "\n",
    "**Corpus:** 3 million lines of English poetry from Project Gutenberg\n",
    "\n",
    "**Source:** Hugging Face - biglam/gutenberg-poetry-corpus\n",
    "\n",
    "**Size:** ~500MB compressed\n",
    "\n",
    "**Output:** Saved to Google Drive at `/MyDrive/gutenberg_poetry_corpus.jsonl.gz`\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Upload this notebook to Google Colab\n",
    "2. Select **Runtime → Change runtime type → GPU: None** (CPU is fine for downloading)\n",
    "3. Run cells in order\n",
    "4. Download will take ~10-15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"✓ Google Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Gutenberg Poetry Corpus\n",
    "\n",
    "This will download the corpus from Hugging Face and save it to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Downloading Gutenberg Poetry Corpus from Hugging Face...\")\n",
    "print(\"This will take ~10-15 minutes\\n\")\n",
    "\n",
    "dataset = load_dataset(\"biglam/gutenberg-poetry-corpus\")\n",
    "\n",
    "print(f\"✓ Dataset loaded: {len(dataset['train']):,} lines of poetry\")\n",
    "print(f\"\\nSample line: {dataset['train'][0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save to Google Drive (Compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output path\n",
    "output_path = \"/content/drive/MyDrive/gutenberg_poetry_corpus.jsonl.gz\"\n",
    "\n",
    "print(f\"Saving to Google Drive: {output_path}\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "# Write compressed JSONL\n",
    "with gzip.open(output_path, 'wt', encoding='utf-8') as f:\n",
    "    for item in tqdm(dataset['train'], desc=\"Writing lines\"):\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"\\n✓ Download complete!\")\n",
    "print(f\"✓ Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify File Size and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check file size\n",
    "file_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "# Read first 5 lines to verify\n",
    "print(\"\\nFirst 5 lines:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with gzip.open(output_path, 'rt', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        print(f\"{i+1}. {data}\")\n",
    "\n",
    "print(\"\\n✓ Corpus ready for BERT training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Create Plain Text Version\n",
    "\n",
    "If you want a plain text file (one line per poem line) instead of JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to create plain text version\n",
    "\n",
    "# output_txt = \"/content/drive/MyDrive/gutenberg_poetry_corpus.txt\"\n",
    "\n",
    "# print(f\"Creating plain text version: {output_txt}\")\n",
    "\n",
    "# with open(output_txt, 'w', encoding='utf-8') as f:\n",
    "#     for item in tqdm(dataset['train'], desc=\"Writing lines\"):\n",
    "#         # Extract just the text of each line\n",
    "#         f.write(item['s'] + '\\n')\n",
    "\n",
    "# print(\"✓ Plain text version created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **View in Google Drive:** Check your Drive to confirm the file is there\n",
    "2. **Prepare for BERT Training:** Use this corpus to train period-specific BERT models\n",
    "3. **Clean/Filter (optional):** You may want to filter by time period or poet\n",
    "\n",
    "---\n",
    "\n",
    "## Corpus Metadata\n",
    "\n",
    "Each line in the corpus contains:\n",
    "- `s`: The line of poetry (string)\n",
    "- `gid`: Project Gutenberg book ID (integer)\n",
    "\n",
    "You can use `gid` to group lines by book/poet or to filter specific works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
