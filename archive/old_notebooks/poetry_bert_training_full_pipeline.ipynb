{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Poetry BERT Training: Complete Pipeline\n\n**This notebook does everything:**\n1. ✅ Build unified poetry database from 4 JSONL sources\n2. ✅ Export 6.2M line training corpus\n3. ✅ Train Poetry BERT on unified corpus\n4. ✅ Save and test trained model\n\n## Before you start:\n\n**All files should be in Google Drive at:** `MyDrive/AI and Poetry/Data/Databases/poetry_corpus/`\n- `shakespeare_complete_works.jsonl`\n- `gutenberg_reconstructed.jsonl`\n- `core_poets_complete.jsonl`\n- `poetrydb.jsonl`\n\n**Estimated total time:** 8-12 hours on Colab GPU\n- Database build: 2-4 hours\n- BERT training: 6-8 hours"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup - Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate tqdm -q\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"✓ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Unified Poetry Database\n",
    "\n",
    "This combines all 4 sources into one SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport sqlite3\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, Optional\nfrom tqdm import tqdm\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n\n# Configure paths - corrected folder name\nDRIVE_BASE = Path(\"/content/drive/MyDrive/AI and Poetry/Data/Databases/poetry_corpus\")\nDB_PATH = \"/content/poetry_unified.db\"\n\nSOURCES = {\n    'shakespeare': DRIVE_BASE / \"shakespeare_complete_works.jsonl\",\n    'gutenberg': DRIVE_BASE / \"gutenberg_reconstructed.jsonl\",\n    'core_poets': DRIVE_BASE / \"core_poets_complete.jsonl\",\n    'poetrydb': DRIVE_BASE / \"poetrydb.jsonl\"\n}\n\nprint(\"Checking source files...\")\nfor name, path in SOURCES.items():\n    if path.exists():\n        size_mb = path.stat().st_size / (1024**2)\n        print(f\"  ✓ {name}: {size_mb:.1f} MB\")\n    else:\n        print(f\"  ✗ {name}: NOT FOUND at {path}\")\n        print(f\"     Please check Google Drive path!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class UnifiedDatabaseBuilder:\n    \"\"\"Build unified SQLite database from all poetry sources.\"\"\"\n\n    def __init__(self, db_path: str):\n        self.db_path = Path(db_path)\n        self.conn = None\n        self.cursor = None\n\n    def normalize_text(self, text: str) -> str:\n        \"\"\"Normalize text for searching.\"\"\"\n        import re\n        text = text.lower()\n        text = re.sub(r'[^\\w\\s]', '', text)\n        return text.strip()\n\n    def generate_work_id(self, source: str, title: str, author: str = \"\", index: int = 0) -> str:\n        \"\"\"Generate a work_id if one doesn't exist.\"\"\"\n        import hashlib\n        # Create a deterministic ID from title + author\n        id_base = f\"{source}_{self.normalize_text(title)}_{self.normalize_text(author)}\"\n        if index > 0:\n            id_base += f\"_{index}\"\n        # Use first 16 chars of hash for uniqueness\n        hash_suffix = hashlib.md5(id_base.encode()).hexdigest()[:8]\n        return f\"{source}_{hash_suffix}\"\n\n    def create_schema(self):\n        \"\"\"Create unified database schema.\"\"\"\n        logging.info(\"Creating database schema...\")\n\n        # Works table\n        self.cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS works (\n                work_id TEXT PRIMARY KEY,\n                title TEXT NOT NULL,\n                author TEXT,\n                first_appearance_date INTEGER,\n                publication_date INTEGER,\n                composition_date INTEGER,\n                period TEXT,\n                genre TEXT,\n                source TEXT NOT NULL,\n                line_count INTEGER,\n                metadata_complete BOOLEAN,\n                full_text TEXT,\n                career_period TEXT,\n                gutenberg_id INTEGER,\n                poetrydb_id TEXT,\n                title_normalized TEXT,\n                author_normalized TEXT\n            )\n        \"\"\")\n\n        # Lines table\n        self.cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS lines (\n                line_id INTEGER PRIMARY KEY AUTOINCREMENT,\n                work_id TEXT NOT NULL,\n                line_num INTEGER NOT NULL,\n                line_text TEXT NOT NULL,\n                is_blank BOOLEAN DEFAULT FALSE,\n                meter TEXT,\n                feet INTEGER,\n                stresses TEXT,\n                rhyme_scheme TEXT,\n                FOREIGN KEY (work_id) REFERENCES works(work_id)\n            )\n        \"\"\")\n\n        # Authors table\n        self.cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS authors (\n                author_id INTEGER PRIMARY KEY AUTOINCREMENT,\n                name TEXT UNIQUE NOT NULL,\n                name_normalized TEXT,\n                birth_year INTEGER,\n                death_year INTEGER,\n                period TEXT,\n                work_count INTEGER\n            )\n        \"\"\")\n\n        # Metadata table\n        self.cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS metadata (\n                work_id TEXT NOT NULL,\n                key TEXT NOT NULL,\n                value TEXT,\n                PRIMARY KEY (work_id, key),\n                FOREIGN KEY (work_id) REFERENCES works(work_id)\n            )\n        \"\"\")\n\n        # Create indexes\n        self.cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_works_author ON works(author)\")\n        self.cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_works_period ON works(period)\")\n        self.cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_works_genre ON works(genre)\")\n        self.cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_works_source ON works(source)\")\n        self.cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_lines_work ON lines(work_id)\")\n        self.cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_lines_text ON lines(line_text)\")\n\n        self.conn.commit()\n        logging.info(\"✓ Schema created\")\n\n    def import_shakespeare(self, input_path: str):\n        \"\"\"Import Shakespeare corpus.\"\"\"\n        input_path = Path(input_path)\n        if not input_path.exists():\n            logging.warning(f\"Shakespeare corpus not found at {input_path}\")\n            return\n\n        logging.info(f\"Importing Shakespeare from {input_path}...\")\n\n        works = []\n        with open(input_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                works.append(json.loads(line))\n\n        generated_ids = 0\n        for idx, work in enumerate(tqdm(works, desc=\"Shakespeare\")):\n            # Generate work_id if missing\n            if 'work_id' not in work or not work['work_id']:\n                work['work_id'] = self.generate_work_id('shakespeare', work['title'], 'William Shakespeare', idx)\n                generated_ids += 1\n\n            career_period = self._guess_shakespeare_career_period(work.get('date'))\n            date = work.get('date')\n            \n            self.cursor.execute(\"\"\"\n                INSERT OR IGNORE INTO works (\n                    work_id, title, author, first_appearance_date, publication_date,\n                    composition_date, period, genre, source, line_count,\n                    metadata_complete, full_text, career_period,\n                    title_normalized, author_normalized\n                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", (\n                work['work_id'], work['title'], 'William Shakespeare',\n                date, date, date, 'early_modern', work.get('genre', 'drama'),\n                work.get('source', 'shakespeare'), work.get('line_count', 0), True, work.get('text'),\n                career_period, self.normalize_text(work['title']),\n                self.normalize_text('William Shakespeare')\n            ))\n\n            if 'lines' in work:\n                for i, line_text in enumerate(work['lines'], 1):\n                    is_blank = len(line_text.strip()) == 0\n                    self.cursor.execute(\"\"\"\n                        INSERT INTO lines (work_id, line_num, line_text, is_blank)\n                        VALUES (?, ?, ?, ?)\n                    \"\"\", (work['work_id'], i, line_text, is_blank))\n\n        self.conn.commit()\n        if generated_ids > 0:\n            logging.info(f\"✓ Imported {len(works)} Shakespeare works ({generated_ids} work_ids generated)\")\n        else:\n            logging.info(f\"✓ Imported {len(works)} Shakespeare works\")\n\n    def _guess_shakespeare_career_period(self, year: Optional[int]) -> str:\n        if not year:\n            return 'unknown'\n        if year < 1594:\n            return 'early'\n        elif year < 1601:\n            return 'middle'\n        elif year < 1608:\n            return 'late'\n        else:\n            return 'final'\n\n    def import_gutenberg(self, input_path: str):\n        \"\"\"Import Gutenberg reconstructed works.\"\"\"\n        input_path = Path(input_path)\n        if not input_path.exists():\n            logging.warning(f\"Gutenberg not found at {input_path}\")\n            return\n\n        logging.info(f\"Importing Gutenberg from {input_path}...\")\n\n        works = []\n        with open(input_path, 'r', encoding='utf-8') as f:\n            for line in tqdm(f, desc=\"Reading Gutenberg\"):\n                works.append(json.loads(line))\n\n        generated_ids = 0\n        for idx, work in enumerate(tqdm(works, desc=\"Gutenberg\")):\n            # Generate work_id if missing\n            if 'work_id' not in work or not work['work_id']:\n                work['work_id'] = self.generate_work_id('gutenberg', work['title'], work.get('author', ''), idx)\n                generated_ids += 1\n\n            pub_date = work.get('publication_date')\n            \n            self.cursor.execute(\"\"\"\n                INSERT OR IGNORE INTO works (\n                    work_id, title, author, first_appearance_date, publication_date,\n                    composition_date, period, genre, source, line_count,\n                    metadata_complete, full_text, gutenberg_id,\n                    title_normalized, author_normalized\n                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", (\n                work['work_id'], work['title'], work.get('author'),\n                pub_date, pub_date, work.get('composition_date'),\n                work.get('period', 'unknown'), 'poetry', work.get('source', 'gutenberg'),\n                work['line_count'], work.get('metadata_complete', False),\n                work.get('text'), work.get('gutenberg_id'),\n                self.normalize_text(work['title']),\n                self.normalize_text(work.get('author', ''))\n            ))\n\n            for i, line_text in enumerate(work['lines'], 1):\n                is_blank = len(line_text.strip()) == 0\n                self.cursor.execute(\"\"\"\n                    INSERT INTO lines (work_id, line_num, line_text, is_blank)\n                    VALUES (?, ?, ?, ?)\n                \"\"\", (work['work_id'], i, line_text, is_blank))\n\n            if 'subjects' in work:\n                for subject in work['subjects']:\n                    self.cursor.execute(\"\"\"\n                        INSERT OR IGNORE INTO metadata (work_id, key, value)\n                        VALUES (?, ?, ?)\n                    \"\"\", (work['work_id'], 'subject', subject))\n\n        self.conn.commit()\n        if generated_ids > 0:\n            logging.info(f\"✓ Imported {len(works)} Gutenberg works ({generated_ids} work_ids generated)\")\n        else:\n            logging.info(f\"✓ Imported {len(works)} Gutenberg works\")\n\n    def import_core_poets(self, input_path: str):\n        \"\"\"Import Core 27 Poets corpus.\"\"\"\n        input_path = Path(input_path)\n        if not input_path.exists():\n            logging.warning(f\"Core poets not found at {input_path}\")\n            return\n\n        logging.info(f\"Importing Core Poets from {input_path}...\")\n\n        works = []\n        with open(input_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                works.append(json.loads(line))\n\n        generated_ids = 0\n        for idx, work in enumerate(tqdm(works, desc=\"Core Poets\")):\n            # Generate work_id if missing\n            if 'work_id' not in work or not work['work_id']:\n                work['work_id'] = self.generate_work_id('core_poets', work['title'], work['author'], idx)\n                generated_ids += 1\n\n            pub_date = work.get('publication_date')\n            \n            self.cursor.execute(\"\"\"\n                INSERT OR IGNORE INTO works (\n                    work_id, title, author, first_appearance_date, publication_date,\n                    composition_date, period, genre, source, line_count,\n                    metadata_complete, full_text, title_normalized, author_normalized\n                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", (\n                work['work_id'], work['title'], work['author'],\n                pub_date, pub_date, work.get('composition_date'),\n                work.get('period', 'unknown'), work.get('genre', 'poetry'),\n                work.get('source', 'core_poets'), work.get('line_count', 0),\n                work.get('metadata_complete', True), work.get('text'),\n                self.normalize_text(work['title']),\n                self.normalize_text(work['author'])\n            ))\n\n            if 'lines' in work:\n                for i, line_text in enumerate(work['lines'], 1):\n                    is_blank = len(line_text.strip()) == 0\n                    self.cursor.execute(\"\"\"\n                        INSERT INTO lines (work_id, line_num, line_text, is_blank)\n                        VALUES (?, ?, ?, ?)\n                    \"\"\", (work['work_id'], i, line_text, is_blank))\n\n        self.conn.commit()\n        if generated_ids > 0:\n            logging.info(f\"✓ Imported {len(works)} Core Poets works ({generated_ids} work_ids generated)\")\n        else:\n            logging.info(f\"✓ Imported {len(works)} Core Poets works\")\n\n    def import_poetrydb(self, input_path: str):\n        \"\"\"Import PoetryDB corpus.\"\"\"\n        input_path = Path(input_path)\n        if not input_path.exists():\n            logging.warning(f\"PoetryDB not found at {input_path}\")\n            return\n\n        logging.info(f\"Importing PoetryDB from {input_path}...\")\n\n        works = []\n        with open(input_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                works.append(json.loads(line))\n\n        generated_ids = 0\n        for idx, work in enumerate(tqdm(works, desc=\"PoetryDB\")):\n            # Generate work_id if missing\n            if 'work_id' not in work or not work['work_id']:\n                work['work_id'] = self.generate_work_id('poetrydb', work['title'], work['author'], idx)\n                generated_ids += 1\n\n            pub_date = work.get('publication_date')\n            \n            self.cursor.execute(\"\"\"\n                INSERT OR IGNORE INTO works (\n                    work_id, title, author, first_appearance_date, publication_date,\n                    period, genre, source, line_count, metadata_complete, full_text,\n                    poetrydb_id, title_normalized, author_normalized\n                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", (\n                work['work_id'], work['title'], work['author'],\n                pub_date, pub_date, work.get('period', 'unknown'),\n                'poetry', work.get('source', 'poetrydb'), work.get('line_count', 0),\n                True, work.get('text'), work.get('poetrydb_id'),\n                self.normalize_text(work['title']),\n                self.normalize_text(work['author'])\n            ))\n\n            if 'lines' in work:\n                for i, line_text in enumerate(work['lines'], 1):\n                    is_blank = len(line_text.strip()) == 0\n                    self.cursor.execute(\"\"\"\n                        INSERT INTO lines (work_id, line_num, line_text, is_blank)\n                        VALUES (?, ?, ?, ?)\n                    \"\"\", (work['work_id'], i, line_text, is_blank))\n\n        self.conn.commit()\n        if generated_ids > 0:\n            logging.info(f\"✓ Imported {len(works)} PoetryDB works ({generated_ids} work_ids generated)\")\n        else:\n            logging.info(f\"✓ Imported {len(works)} PoetryDB works\")\n\n    def build_author_table(self):\n        \"\"\"Populate authors table from works.\"\"\"\n        logging.info(\"Building authors table...\")\n\n        self.cursor.execute(\"\"\"\n            INSERT OR IGNORE INTO authors (name, name_normalized, work_count)\n            SELECT author, author_normalized, COUNT(*) as work_count\n            FROM works\n            WHERE author IS NOT NULL\n            GROUP BY author, author_normalized\n        \"\"\")\n\n        self.conn.commit()\n        author_count = self.cursor.execute(\"SELECT COUNT(*) FROM authors\").fetchone()[0]\n        logging.info(f\"✓ Built authors table with {author_count} authors\")\n\n    def print_summary(self):\n        \"\"\"Print database summary statistics.\"\"\"\n        logging.info(\"\\n\" + \"=\"*60)\n        logging.info(\"DATABASE SUMMARY\")\n        logging.info(\"=\"*60)\n\n        total_works = self.cursor.execute(\"SELECT COUNT(*) FROM works\").fetchone()[0]\n        total_lines = self.cursor.execute(\"SELECT COUNT(*) FROM lines\").fetchone()[0]\n        total_authors = self.cursor.execute(\"SELECT COUNT(*) FROM authors\").fetchone()[0]\n\n        logging.info(f\"\\nTotal works: {total_works:,}\")\n        logging.info(f\"Total lines: {total_lines:,}\")\n        logging.info(f\"Total authors: {total_authors:,}\")\n\n        logging.info(\"\\nBy source:\")\n        sources = self.cursor.execute(\"\"\"\n            SELECT source, COUNT(*), SUM(line_count)\n            FROM works\n            GROUP BY source\n            ORDER BY COUNT(*) DESC\n        \"\"\").fetchall()\n\n        for source, count, lines in sources:\n            logging.info(f\"  {source}: {count:,} works, {lines:,} lines\")\n\n        logging.info(\"\\n\" + \"=\"*60)\n\n    def build(self, sources: Dict[str, str]):\n        \"\"\"Build the unified database from all sources.\"\"\"\n        logging.info(f\"Creating database at {self.db_path}...\")\n        \n        # Delete existing database if it exists\n        if self.db_path.exists():\n            self.db_path.unlink()\n            logging.info(\"✓ Deleted existing database\")\n        \n        self.conn = sqlite3.connect(self.db_path)\n        self.cursor = self.conn.cursor()\n\n        try:\n            self.create_schema()\n\n            # Import in order: Shakespeare, Gutenberg, Core Poets, PoetryDB\n            if 'shakespeare' in sources:\n                self.import_shakespeare(sources['shakespeare'])\n\n            if 'gutenberg' in sources:\n                self.import_gutenberg(sources['gutenberg'])\n\n            if 'core_poets' in sources:\n                self.import_core_poets(sources['core_poets'])\n\n            if 'poetrydb' in sources:\n                self.import_poetrydb(sources['poetrydb'])\n\n            self.build_author_table()\n            self.print_summary()\n\n            logging.info(f\"\\n✓ Database built successfully: {self.db_path}\")\n\n        finally:\n            self.conn.close()\n\nprint(\"✓ UnifiedDatabaseBuilder class defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Database Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the database\n",
    "builder = UnifiedDatabaseBuilder(DB_PATH)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BUILDING UNIFIED POETRY DATABASE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "\n",
    "builder.build(SOURCES)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUILD COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Export Training Corpus\n",
    "\n",
    "Export all non-blank lines from the database as a text file for BERT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "CORPUS_OUTPUT = \"/content/poetry_training_corpus.txt\"\n",
    "\n",
    "print(\"Exporting training corpus...\")\n",
    "print(f\"Database: {DB_PATH}\")\n",
    "print(f\"Output: {CORPUS_OUTPUT}\")\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get total line count\n",
    "total_lines = cursor.execute(\"SELECT COUNT(*) FROM lines WHERE is_blank = 0\").fetchone()[0]\n",
    "print(f\"\\nExporting {total_lines:,} non-blank lines...\")\n",
    "\n",
    "# Export lines\n",
    "with open(CORPUS_OUTPUT, 'w', encoding='utf-8') as f:\n",
    "    cursor.execute(\"SELECT line_text FROM lines WHERE is_blank = 0 ORDER BY line_id\")\n",
    "    \n",
    "    batch_size = 10000\n",
    "    pbar = tqdm(total=total_lines, desc=\"Exporting\")\n",
    "    \n",
    "    while True:\n",
    "        rows = cursor.fetchmany(batch_size)\n",
    "        if not rows:\n",
    "            break\n",
    "        \n",
    "        for (line_text,) in rows:\n",
    "            f.write(line_text + '\\n')\n",
    "        \n",
    "        pbar.update(len(rows))\n",
    "    \n",
    "    pbar.close()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "size_gb = os.path.getsize(CORPUS_OUTPUT) / (1024**3)\n",
    "print(f\"\\n✓ Corpus exported: {size_gb:.2f} GB\")\n",
    "print(f\"✓ Location: {CORPUS_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure BERT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "SAVE_STEPS = 1000\n",
    "LOGGING_STEPS = 100\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Save every: {SAVE_STEPS} steps\")\n",
    "print(f\"  Log every: {LOGGING_STEPS} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Load and Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading BERT model...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "print(f\"✓ Model loaded: {model.num_parameters():,} parameters\")\n",
    "\n",
    "# Load corpus\n",
    "print(f\"\\nLoading corpus from {CORPUS_OUTPUT}...\")\n",
    "dataset = load_dataset('text', data_files={'train': CORPUS_OUTPUT}, split='train')\n",
    "print(f\"✓ Loaded {len(dataset):,} lines\")\n",
    "\n",
    "# Tokenize\n",
    "print(\"\\nTokenizing corpus...\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "print(\"✓ Tokenization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Data collator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./poetry_bert_checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # Mixed precision\n",
    "    logging_dir='./logs',\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"✓ Training setup complete\")\n",
    "print(f\"Total training steps: {len(tokenized_dataset) // BATCH_SIZE * NUM_EPOCHS:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Start Training (6-8 hours)\n",
    "\n",
    "⚠️ **This will take 6-8 hours on Colab GPU**\n",
    "\n",
    "You can close the tab - training will continue in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING POETRY BERT TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Estimated time: 6-8 hours on Colab GPU\")\n",
    "print(\"You can close this tab - training will continue\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n✓ Training complete! Total time: {total_time/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to local Colab storage\n",
    "output_dir = \"./poetry_bert_trained\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"✓ Model saved to {output_dir}\")\n",
    "\n",
    "# Also save to Google Drive\n",
    "drive_output = \"/content/drive/MyDrive/poetry_bert_trained\"\n",
    "!cp -r {output_dir} {drive_output}\n",
    "print(f\"✓ Model also saved to Google Drive: {drive_output}\")\n",
    "\n",
    "# Save the database too\n",
    "!cp {DB_PATH} /content/drive/MyDrive/poetry_unified.db\n",
    "print(f\"✓ Database saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the trained model\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test on Shakespeare\n",
    "test_text = \"Shall I compare thee to a [MASK] day?\"\n",
    "print(f\"Test: '{test_text}'\\n\")\n",
    "\n",
    "predictions = fill_mask(test_text)\n",
    "for i, pred in enumerate(predictions[:5], 1):\n",
    "    print(f\"{i}. {pred['token_str']}: {pred['score']:.4f}\")\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "# Test on Modernist-style line\n",
    "test_text2 = \"The [MASK] hangs in fragments over the broken town\"\n",
    "print(f\"Test: '{test_text2}'\\n\")\n",
    "\n",
    "predictions2 = fill_mask(test_text2)\n",
    "for i, pred in enumerate(predictions2[:5], 1):\n",
    "    print(f\"{i}. {pred['token_str']}: {pred['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Download Files (Optional)\n",
    "\n",
    "Download the trained model and database to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model for easier download\n",
    "!zip -r poetry_bert_trained.zip poetry_bert_trained/\n",
    "\n",
    "# Download via Colab files panel (left sidebar)\n",
    "from google.colab import files\n",
    "\n",
    "print(\"To download:\")\n",
    "print(\"  1. Model: poetry_bert_trained.zip (~500MB)\")\n",
    "print(\"  2. Database: poetry_unified.db (~3-4GB)\")\n",
    "print(\"\\nUse the Files panel on the left to download, or run:\")\n",
    "print(\"  files.download('poetry_bert_trained.zip')\")\n",
    "print(\"  files.download('poetry_unified.db')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ Built unified poetry database (4,444 works, 6.2M lines)\n",
    "✅ Exported training corpus\n",
    "✅ Trained Poetry BERT\n",
    "✅ Saved model to Google Drive\n",
    "\n",
    "**Next steps:**\n",
    "1. Download model from Google Drive\n",
    "2. Use for semantic trajectory analysis\n",
    "3. Extract embeddings for specific poems\n",
    "4. Compare with EEBO-BERT for diachronic analysis"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}