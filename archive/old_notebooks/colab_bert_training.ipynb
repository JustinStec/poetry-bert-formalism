{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEBO-BERT Fine-Tuning on Google Colab\n",
    "\n",
    "**Steps:**\n",
    "1. Upload `eebo_cleaned_corpus.txt` (7.6GB) to Google Drive\n",
    "2. Run all cells below\n",
    "3. Download trained model when complete\n",
    "\n",
    "**Estimated time:** 6-8 hours on Colab GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (to access corpus file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to where you uploaded the corpus in Google Drive\n",
    "CORPUS_PATH = \"/content/drive/MyDrive/eebo_cleaned_corpus.txt\"\n",
    "\n",
    "# Training settings\n",
    "BATCH_SIZE = 8  # Colab GPUs have more memory\n",
    "MAX_LENGTH = 512\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "SAVE_STEPS = 1000\n",
    "LOGGING_STEPS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading BERT model...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "print(f\"✓ Model loaded: {model.num_parameters():,} parameters\")\n",
    "\n",
    "# Load corpus\n",
    "print(f\"\\nLoading corpus from {CORPUS_PATH}...\")\n",
    "dataset = load_dataset('text', data_files={'train': CORPUS_PATH}, split='train')\n",
    "print(f\"✓ Loaded {len(dataset):,} lines\")\n",
    "\n",
    "# Tokenize\n",
    "print(\"\\nTokenizing corpus...\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "print(\"✓ Tokenization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Data collator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # Use mixed precision for speed\n",
    "    logging_dir='./logs',\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"✓ Training setup complete\")\n",
    "print(f\"Total training steps: {len(tokenized_dataset) // BATCH_SIZE * NUM_EPOCHS:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start Training (This will take 6-8 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(\"This will take approximately 6-8 hours.\")\n",
    "print(\"You can close this tab - training will continue.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n✓ Training complete! Total time: {total_time/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to local Colab storage\n",
    "output_dir = \"./eebo_bert_finetuned\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"✓ Model saved to {output_dir}\")\n",
    "\n",
    "# Also save to Google Drive\n",
    "drive_output = \"/content/drive/MyDrive/eebo_bert_finetuned\"\n",
    "!cp -r {output_dir} {drive_output}\n",
    "print(f\"✓ Model also saved to Google Drive: {drive_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the trained model\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test\n",
    "test_text = \"Shall I compare thee to a [MASK] day?\"\n",
    "print(f\"Test: '{test_text}'\\n\")\n",
    "\n",
    "predictions = fill_mask(test_text)\n",
    "for i, pred in enumerate(predictions[:5], 1):\n",
    "    print(f\"{i}. {pred['token_str']}: {pred['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Model (Optional)\n",
    "\n",
    "To download the trained model to your computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model for easier download\n",
    "!zip -r eebo_bert_finetuned.zip eebo_bert_finetuned/\n",
    "\n",
    "# Download via Colab files panel (left sidebar)\n",
    "from google.colab import files\n",
    "files.download('eebo_bert_finetuned.zip')\n",
    "\n",
    "print(\"✓ Model zip file ready for download!\")\n",
    "print(\"Check your Downloads folder or use the Files panel on the left.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
